"""Copyright 2022 Google LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    https://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 60K\r\n",
      "drwxr-x--- 2 agoldie primarygroup  12K Jan 20 11:47 afp_eng\r\n",
      "drwxr-x--- 2 agoldie primarygroup  12K Jan 20 11:48 apw_eng\r\n",
      "drwxr-x--- 2 agoldie primarygroup 4.0K Jan 20 11:48 cna_eng\r\n",
      "drwxr-x--- 2 agoldie primarygroup 4.0K Jan 20 11:49 ltw_eng\r\n",
      "drwxr-x--- 2 agoldie primarygroup  12K Jan 20 11:50 nyt_eng\r\n",
      "drwxr-x--- 2 agoldie primarygroup 4.0K Jan 20 11:50 wpb_eng\r\n",
      "drwxr-x--- 2 agoldie primarygroup  12K Jan 20 11:50 xin_eng\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh /usr/local/google/home/agoldie/data/gigaword/LDC2011T07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a tiny test file to quickly develop end-to-end script.\n",
    "!head -n 100 ~/data/gigaword/LDC2011T07/afp_eng/afp_eng_199406 > /tmp/short_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's maintain a file in which we store our progress, a clear record of exactly how much of\n",
    "# the gigaword dataset has been incorporated into the final product (i.e. the word files).\n",
    "# /usr/local/google/home/agoldie/data/gigaword/data_generation_progress.txt\n",
    "!cat /usr/local/google/home/agoldie/data/gigaword/data_generation_progress.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def extract_sentences(file):\n",
    "    sentences = []\n",
    "    sentence = \"\"\n",
    "    with open(file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if '<' in line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = \"\"\n",
    "            else:\n",
    "                sentence = line if not sentence else sentence + \" \" + line\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def write_train_tsv(output_dir, shard, sentences):\n",
    "    output_file = os.path.join(output_dir, shard + '.tsv')\n",
    "    with open(output_file, 'w') as f:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            f.write('\\t'.join([str(i), '1', shard, sentence, '\\n']))\n",
    "    return output_file\n",
    "            \n",
    "def load_processed_shards(progress_log='/usr/local/google/home/agoldie/data/gigaword/data_generation_progress.txt'):\n",
    "    with open(progress_log, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "        return set(lines)\n",
    "\n",
    "# Utility cell to check sanity by making context human readable.\n",
    "def load_id_to_word_map(file):\n",
    "      id_to_word_map = {}\n",
    "      word_to_id_map = {}\n",
    "      with open(file, 'r', errors='replace') as f:\n",
    "        for i, word in enumerate(f):\n",
    "          word = word.strip()\n",
    "          id_to_word_map[i] = word\n",
    "          word_to_id_map[word] = i\n",
    "      return id_to_word_map, word_to_id_map\n",
    "    \n",
    "def filter_word_pieces(input_file, output_file):\n",
    "    lines_to_retain = []\n",
    "    previous = {}\n",
    "    with open(input_file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            segments = line.split('][')\n",
    "            if len(segments) != 4:\n",
    "                print('not 4', len(segments), line)\n",
    "                continue\n",
    "            word_id, _, _, _ = line.split('][')\n",
    "            word_id = word_id.strip('[')\n",
    "            word_id = int(word_id)\n",
    "            word = id_to_word_map[word_id]\n",
    "            # 'Full word' token that should be retained and aggregated.\n",
    "            if previous and not word.startswith('##') and not previous['word'].startswith('##'):\n",
    "                lines_to_retain.append(previous['line'])\n",
    "            previous = {'line': line, 'word': word}\n",
    "        # Always add the last token if it isn't prefixed with '##'.\n",
    "        if previous and not previous['word'].startswith('##'):\n",
    "            lines_to_retain.append(previous['line'])\n",
    "\n",
    "    #print(len(lines_to_retain), lines_to_retain[0])       \n",
    "    with open(output_file, 'w') as o:\n",
    "        for line in lines_to_retain:\n",
    "            o.write(line)\n",
    "    \n",
    "def write_word_files(input_file, output_dir, id_to_word_map, shard):\n",
    "    with open(input_file, 'r') as f:\n",
    "        embeddings = []\n",
    "        contexts = []\n",
    "        positions = []\n",
    "        current_id = ''\n",
    "        for i, line in enumerate(f):\n",
    "            if i and i % 100000 == 0:\n",
    "                print('Processed', str(i), 'lines so far...')\n",
    "            # First, parse all the information on the line:\n",
    "            # [WORD_ID][POS][CONTEXT][EMBEDDING]\\n\n",
    "            word_id, pos, context, embedding = line.split('][')\n",
    "            word_id = int(word_id.strip('['))\n",
    "            embedding = embedding.strip().strip(']')\n",
    "\n",
    "            # If it's not the first line and the word is a new one, then write to that word's file.\n",
    "            # The output format is POS\\tCONTEXT\\tEMBEDDING\\n\n",
    "            if not current_id:\n",
    "                current_id = word_id\n",
    "            elif word_id != current_id:\n",
    "                word = id_to_word_map[current_id]\n",
    "                word_file = output_dir + word + '.txt'\n",
    "                assert(len(embeddings) == len(contexts))\n",
    "                assert(len(contexts) == len(positions))\n",
    "                with open(word_file, 'a') as a:\n",
    "                    for i in range(len(embeddings)):\n",
    "                        output_line = '\\t'.join([positions[i], contexts[i], embeddings[i], shard]) + '\\n'\n",
    "                        a.write(output_line)\n",
    "                # Update current word and empty arrays for the (previously) current_word.\n",
    "                # These arrays will be populated later on.\n",
    "                current_id = word_id\n",
    "                embeddings = []\n",
    "                contexts = []\n",
    "                positions = []\n",
    "\n",
    "            # Persist this information in arrays until it's time to write it in a block to one file.\n",
    "            embeddings.append(embedding)\n",
    "            contexts.append(context)\n",
    "            positions.append(pos)\n",
    "\n",
    "        # Flush out last word from arrays!\n",
    "        if current_id in id_to_word_map:\n",
    "            word = id_to_word_map[current_id]\n",
    "            word_file = output_dir + word + '.txt'\n",
    "            #print('writing to', word_file)\n",
    "            assert(len(embeddings) == len(contexts))\n",
    "            assert(len(contexts) == len(positions))\n",
    "            with open(word_file, 'a') as a:\n",
    "                for i in range(len(embeddings)):\n",
    "                    output_line = '\\t'.join([positions[i], contexts[i], embeddings[i], shard]) + '\\n'\n",
    "                    a.write(output_line)\n",
    "        else:\n",
    "            print(current_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping previously processed xin_eng_199502 ...\n",
      "Skipping previously processed xin_eng_199501 ...\n",
      "Skipping previously processed apw_eng_199502 ...\n",
      "Skipping previously processed apw_eng_199503 ...\n"
     ]
    }
   ],
   "source": [
    "# TODO: Filter out all entries that are not alphabetic (e.g. '(' and ')') to save more space!\n",
    "# Save those in cns for later?\n",
    "# TODO: Remove after testing!\n",
    "progress_log = '/tmp/progress_log.txt'\n",
    "processed_shards = load_processed_shards(progress_log)\n",
    "\n",
    "# Load vocabulary\n",
    "vocab_file = '/usr/local/google/home/agoldie/data/uncased_L-12_H-768_A-12/vocab.txt'\n",
    "id_to_word_map, _ = load_id_to_word_map(vocab_file)\n",
    "    \n",
    "#data_dir = '/usr/local/google/home/agoldie/data/gigaword/LDC2011T07/'\n",
    "# TODO: Remove after testing!\n",
    "input_dir = '/tmp/test_tree/'\n",
    "output_dir = '/tmp/test_tree_output/'\n",
    "datasets = os.listdir(input_dir)\n",
    "\n",
    "run_bert_template = 'blaze build -c opt third_party/py/language/bert:run_classifier && \\\n",
    "    ./blaze-out/k8-py2-opt/bin/third_party/py/language/bert/run_classifier \\\n",
    "    --do_eval True \\\n",
    "    --data_dir {} \\\n",
    "    --task_name cola \\\n",
    "    --vocab_file uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "    --bert_config_file uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "    --output_dir /tmp/mrpc_output/ \\\n",
    "    --init_checkpoint uncased_L-12_H-768_A-12/bert_model.ckpt &> \\\n",
    "    {}'\n",
    "\n",
    "filter_padding_template = 'grep -v \\'{}\\' {} | grep -v \\'{}\\' | grep -v \\'{}\\' > {}'\n",
    "\n",
    "sort_template = 'sort -k1.2n -s {} > {}'\n",
    "\n",
    "for dataset in datasets:\n",
    "    shards = os.listdir(os.path.join(input_dir, dataset))\n",
    "    for shard in shards:\n",
    "        input_file = os.path.join(input_dir, dataset, shard)\n",
    "        if input_file in processed_shards:\n",
    "            print('Skipping previously processed', os.path.basename(input_file), '...')\n",
    "            continue\n",
    "        sentences = extract_sentences(input_file)\n",
    "        # Generate tsv input files.\n",
    "        tsv_file = write_train_tsv(output_dir, shard, sentences)\n",
    "        # Change to google3 directory to run BERT.\n",
    "        os.chdir('/google/src/cloud/agoldie/simlex/google3')\n",
    "        initial_embeddings = os.path.join(output_dir, shard + '.txt')\n",
    "        run_bert_cmd = run_bert_template.format(tsv_file, initial_embeddings, '/tmp/error')\n",
    "        # Generate BERT embeddings.\n",
    "        os.system(run_bert_cmd)\n",
    "        # Change back to home directory after running BERT.\n",
    "        os.chdir('/usr/local/google/home/agoldie')\n",
    "        filter_args = ['^\\[101\\]', '^\\[102\\]', '^\\[0\\]']\n",
    "        filter_args.insert(1, initial_embeddings)\n",
    "        # Filter out padding (e.g. [PAD], [SEP], [CLS]).\n",
    "        filtered_embeddings = os.path.join(output_dir, 'filtered-' + shard + '.txt')\n",
    "        filter_args.append(filtered_embeddings)\n",
    "        filter_padding_cmd = filter_padding_template.format(*filter_args)\n",
    "        os.system(filter_padding_cmd)\n",
    "        full_embeddings = os.path.join(output_dir, 'full-' + shard + '.txt')\n",
    "        # Filter out all partial word pieces. \n",
    "        filter_word_pieces(filtered_embeddings, full_embeddings)\n",
    "        # Sort by surface form to speed up aggregation.\n",
    "        sorted_embeddings = os.path.join(output_dir, 'sorted-' + shard + '.txt')\n",
    "        sort_cmd = sort_template.format(full_embeddings, sorted_embeddings)\n",
    "        os.system(sort_cmd)\n",
    "        # Write out word files. Update lines to add data source for easy rollbacks.\n",
    "        output_word_dir = os.path.join(output_dir, 'words/')\n",
    "        write_word_files(sorted_embeddings, output_word_dir, id_to_word_map, shard)\n",
    "        # Clean up intermediate files to save space.\n",
    "        #intermediate_files = [initial_embeddings, filtered_embeddings, full_embeddings, sorted_embeddings]\n",
    "        intermediate_files = [filtered_embeddings, full_embeddings, sorted_embeddings]\n",
    "        for file in intermediate_files:\n",
    "            os.remove(file)\n",
    "        # Update progress log to avoid recomputation.\n",
    "        with open(progress_log, 'a') as f:\n",
    "            f.write(input_file + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] na ##ido ##o is minister without portfolio , charged with implementing the an ##c ' s ambitious reconstruction and development programme , which aims to red ##dre ##ss in ##e ##qual ##ities between blacks and whites and boost the economy . [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Utility cell to check sanity by making context human readable.\n",
    "def load_id_to_word_map(file):\n",
    "      id_to_word_map = {}\n",
    "      word_to_id_map = {}\n",
    "      with open(file, 'r', errors='replace') as f:\n",
    "        for i, word in enumerate(f):\n",
    "          word = word.strip()\n",
    "          id_to_word_map[i] = word\n",
    "          word_to_id_map[word] = i\n",
    "      return id_to_word_map, word_to_id_map\n",
    "\n",
    "vocab_file = '/usr/local/google/home/agoldie/data/uncased_L-12_H-768_A-12/vocab.txt'\n",
    "id_to_word_map, word_to_id_map = load_id_to_word_map(vocab_file)\n",
    "input = '101 6583 13820 2080 2003 2704 2302 11103 1010 5338 2007 14972 1996 2019 2278 1005 1055 12479 8735 1998 2458 4746 1010 2029 8704 2000 2417 16200 4757 1999 2063 26426 6447 2090 10823 1998 12461 1998 12992 1996 4610 1012 102'\n",
    "word_ids = list(map(int, input.split()))\n",
    "#print(id_to_word_map[1007])\n",
    "print(' '.join((id_to_word_map[word_id] for word_id in word_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120322\t1\tafp_eng_199406\tSaying he had made three trips to the United States in 17 years, Wattana indicated he had no plans to go again because his children had finished their studies there and \"I'm not fascinated by the United States.\"\t\r\n"
     ]
    }
   ],
   "source": [
    "!grep -i 'saying he had made three trips to the united states in 17 years' /tmp/afp_eng_199406.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Filter out all [CLS], [SEP], and [PAD] tokens.\n",
    "# The number of embeddings went from ~17.4M to ~4.26M!\n",
    "# 3.7M\n",
    "!grep -v '^\\[101\\]' ~/data/gigaword/embeddings/truncated-afp_eng_199406.txt \\\n",
    "| grep -v '^\\[102\\]' | grep -v '^\\[0\\]' \\\n",
    "> ~/data/gigaword/embeddings/filtered-afp_eng_199406.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [NON-TRIVIAL] Step 5 and 6: Filter out all tokens that are not themselves complete words.\n",
    "# Note that ~/data/gigaword/embeddings/filtered-afp_eng_199405-reordered.txt contains ~4.26M tokens\n",
    "# This step (only) reduced the number of embeddings to ~3.69M tokens.\n",
    "#input_file = '/usr/local/google/home/agoldie/data/gigaword/embeddings/filtered-afp_eng_199406.txt'\n",
    "#output_file = '/usr/local/google/home/agoldie/data/gigaword/embeddings/full-afp_eng_199406.txt'\n",
    "\n",
    "lines_to_retain = []\n",
    "#previous = {}\n",
    "with open(input_file, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        segments = line.split('][')\n",
    "        if len(segments) != 4:\n",
    "            print(line)\n",
    "            continue\n",
    "        word_id, _, _, _ = line.split('][')\n",
    "        word_id = word_id.strip('[')\n",
    "        word_id = int(word_id)\n",
    "        word = id_to_word_map[word_id]\n",
    "        # 'Full word' token that should be retained and aggregated.\n",
    "        if previous and not word.startswith('##') and not previous['word'].startswith('##'):\n",
    "            lines_to_retain.append(previous['line'])\n",
    "        previous = {'line': line, 'word': word}\n",
    "    # Always add the last token if it isn't prefixed with '##'.\n",
    "    if previous and not previous['word'].startswith('##'):\n",
    "        lines_to_retain.append(previous['line'])\n",
    "\n",
    "print(len(lines_to_retain), lines_to_retain[0])       \n",
    "with open(output_file, 'w') as o:\n",
    "    for line in lines_to_retain:\n",
    "        o.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6.5: Sort the embeddings file.\n",
    "# Below sorts the embedding file by the first column.\n",
    "# Note that the purpose of sorting is just to aggregate tokens, so it doesn't need to be a numeric sort.\n",
    "# And fortunately sorting did not affect the size of the file, still ~3.69M.\n",
    "!sort -k1.2n -s ~/data/gigaword/embeddings/full-afp_eng_199405.txt > ~/data/gigaword/embeddings/sorted-afp_eng_199405.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Aggregate embeddings by surface form (i.e. one file per surface form).\n",
    "# Code is in generate_embedding_clouds.py\n",
    "# Now generate files for each word in the vocabulary.\n",
    "# TODO: Write these out in binary format that I can store more data!\n",
    "sorted_file = '/usr/local/google/home/agoldie/data/gigaword/embeddings/sorted-afp_eng_199405.txt'\n",
    "output_dir = '/usr/local/google/home/agoldie/data/gigaword/words/'\n",
    "vocab_file = '/usr/local/google/home/agoldie/data/uncased_L-12_H-768_A-12/vocab.txt'\n",
    "id_to_word_map, _ = load_id_to_word_map(vocab_file)\n",
    "\n",
    "with open(sorted_file, 'r') as f:\n",
    "    embeddings = []\n",
    "    contexts = []\n",
    "    positions = []\n",
    "    current_id = ''\n",
    "    for i, line in enumerate(f):\n",
    "        if i and i % 100000 == 0:\n",
    "            print('Processed', str(i), 'lines so far...')\n",
    "        # First, parse all the information on the line:\n",
    "        # [WORD_ID][POS][CONTEXT][EMBEDDING]\\n\n",
    "        word_id, pos, context, embedding = line.split('][')\n",
    "        word_id = int(word_id.strip('['))\n",
    "        embedding = embedding.strip().strip(']')\n",
    "        \n",
    "        # If it's not the first line and the word is a new one, then write to that word's file.\n",
    "        # The output format is POS\\tCONTEXT\\tEMBEDDING\\n\n",
    "        if not current_id:\n",
    "            current_id = word_id\n",
    "        elif word_id != current_id:\n",
    "            word = id_to_word_map[current_id]\n",
    "            word_file = output_dir + word + '.txt'\n",
    "            #print('writing to', word_file)\n",
    "            assert(len(embeddings) == len(contexts))\n",
    "            assert(len(contexts) == len(positions))\n",
    "            with open(word_file, 'a') as a:\n",
    "                for i in range(len(embeddings)):\n",
    "                    output_line = '\\t'.join([positions[i], contexts[i], embeddings[i]]) + '\\n'\n",
    "                    a.write(output_line)\n",
    "            # Update current word and empty arrays for the (previously) current_word.\n",
    "            # These arrays will be populated later on.\n",
    "            current_id = word_id\n",
    "            embeddings = []\n",
    "            contexts = []\n",
    "            positions = []\n",
    "             \n",
    "        # Persist this information in arrays until it's time to write it in a block to one file.\n",
    "        embeddings.append(embedding)\n",
    "        contexts.append(context)\n",
    "        positions.append(pos)\n",
    "    \n",
    "    # Flush out last word from arrays!\n",
    "    word = id_to_word_map[current_id]\n",
    "    word_file = output_dir + word + '.txt'\n",
    "    #print('writing to', word_file)\n",
    "    assert(len(embeddings) == len(contexts))\n",
    "    assert(len(contexts) == len(positions))\n",
    "    with open(word_file, 'a') as a:\n",
    "        for i in range(len(embeddings)):\n",
    "            output_line = '\\t'.join([positions[i], contexts[i], embeddings[i]]) + '\\n'\n",
    "            a.write(output_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up intermediate files once I am satisfied that the word files\n",
    "# have been correctly generated.\n",
    "import os\n",
    "dataset = 'afp_eng_199405'\n",
    "data_dir = '/usr/local/google/home/agoldie/data/gigaword/embeddings/'\n",
    "initial_embeddings = os.path.join(data_dir, dataset + '.txt')\n",
    "filtered_embeddings = os.path.join(data_dir, 'filtered-' + dataset + '.txt')\n",
    "full_embeddings = os.path.join(data_dir, 'full-' + dataset + '.txt')\n",
    "sorted_embeddings = os.path.join(data_dir, 'sorted-' + dataset + '.txt')\n",
    "embeddings_to_remove = [initial_embeddings, filtered_embeddings, full_embeddings, sorted_embeddings]\n",
    "for embedding_file in embeddings_to_remove:\n",
    "    os.remove(embedding_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write entry in progress log.\n",
    "progress_log = '/usr/local/google/home/agoldie/data/gigaword/data_generation_progress.txt'\n",
    "with open(progress_log, 'a') as f:\n",
    "    f.write(input_file + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "list_files = subprocess.run([\"ls\", \"-lh\"], stdout=subprocess.PIPE)\n",
    "#print(\"The exit code was: %d\" % list_files.returncode)\n",
    "#print(list_files.stdout)\n",
    "pwd = subprocess.run([\"pwd\"], stdout=subprocess.PIPE)\n",
    "print(pwd.stdout)\n",
    "os.chdir('/tmp')\n",
    "#cd = subprocess.run([\"cd /usr/local/google/home/agoldie\"])\n",
    "pwd = subprocess.run([\"pwd\"], stdout=subprocess.PIPE)\n",
    "print(pwd.stdout)\n",
    "print(subprocess.run(['pwd'], stdout=subprocess.PIPE).stdout.decode().strip())\n",
    "os.chdir('/usr/local/google/home/agoldie')\n",
    "print(subprocess.run(['pwd'], stdout=subprocess.PIPE).stdout.decode().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 1 ~/data/gigaword/embeddings/afp_eng_199405.txt | cut -c1-334"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
